{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_threshold(nd_array, dim, threshold):\n",
    "    ''' Returns a matrix with zscored and thresholded values along \n",
    "        the specified directions.\n",
    "    :param: nd_array - data\n",
    "            dim - dimensions to z-transform over as list or int\n",
    "            threshold - threshold value\n",
    "    :return: max - zscored and thresholded array with the original nd_array dimensions\n",
    "    '''\n",
    "    print(nd_array.shape)\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    dims = np.arange(len(nd_array.shape))\n",
    "    dimleft = np.setdiff1d(dims, dim)\n",
    "    if not isinstance(dim,list):\n",
    "        dim=[dim]\n",
    "    new_order = [*dim, *dimleft]\n",
    "    mat = np.transpose(nd_array,tuple(new_order))\n",
    "    incl = [nd_array.shape[i] for i in dim]\n",
    "    new = [nd_array.shape[i] for i in dimleft]\n",
    "    new.insert(0, np.prod(incl))\n",
    "    mat = np.reshape(mat, new, order='F')\n",
    "    mat = stats.zscore(mat, axis=0)\n",
    "    mat[mat>threshold] = threshold\n",
    "    mat[mat<-threshold] = -threshold\n",
    "    new = [nd_array.shape[i] for i in dimleft]\n",
    "    new = incl + new\n",
    "    mat = np.reshape(mat,tuple(new), order='F')\n",
    "    new_order=np.argsort(new_order,axis=0)\n",
    "    mat = np.transpose(mat, (new_order))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_average(epochs, num_trials, num_blocks, kind, zscore = False, threshold = False):\n",
    "    ''' Z-scores within time X els, replaces threshshold (if provided) and averages \n",
    "        trials within blocks. \n",
    "    :param epochs: epochs structure\n",
    "    :param num_trials: number of trials to average\n",
    "    :param num_blocks: number of blochs in the data\n",
    "    :param items_n: number of unique triggers\n",
    "    :param zscore: to perform zscoring across time X els. Default - False\n",
    "    :param threshold: the threshold absolute z-value. Everything above is threshold, \n",
    "                        under is -threshold. Default - False\n",
    "    :return epochs: - MNE epochs structure \n",
    "    \n",
    "    >>> epochs_02 = block_average(epochs_02,4,11, zscore = True, threshold = 3)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    import mne\n",
    "    import numpy as np\n",
    "    \n",
    "    if kind == 'perc':\n",
    "        items_n = 100\n",
    "    else:\n",
    "        items_n = 5\n",
    "    print('Starting averaging')\n",
    "    df = epochs.to_data_frame()\n",
    "    df = df.unstack(level = -1)\n",
    "    df['block']=np.tile(np.arange(1,num_blocks+1).repeat(num_trials),items_n)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['condition']=df['condition'].apply(pd.to_numeric)\n",
    "    if zscore:\n",
    "        arr = df.iloc[:,2:-1].values\n",
    "        arr = stats.zscore(arr,axis=1)\n",
    "        if threshold:\n",
    "            arr[np.where(arr>threshold)]=threshold\n",
    "            arr[np.where(arr<-threshold)]=-threshold\n",
    "        else:\n",
    "            print('No thresholding performed')\n",
    "        df.iloc[:,2:-1]=arr\n",
    "    else:\n",
    "        print('No zscoring performed')\n",
    "    df=df.groupby(['block','condition']).mean()\n",
    "    df.reset_index(inplace=True)\n",
    "    data = np.array(df.iloc[:,3:].values)\n",
    "    data = data.reshape(data.shape[0],64,int(data.shape[1]/64))\n",
    "    \n",
    "    if kind == 'perc':\n",
    "        east=list(np.arange(101,126))+list(np.arange(201,226))\n",
    "        df['orientation']=np.where(df['condition']>200,'inv','up')\n",
    "        df['origin']=np.where(df['condition'].isin(east),'east','west')\n",
    "        trigs=list(range(101, 151))+list(range(201, 251))\n",
    "    else: \n",
    "        trigs=list(range(31, 36))\n",
    "    event_ids={str(x):x for x in trigs}\n",
    "        \n",
    "    # Initialize an info structure\n",
    "    events = np.array([np.arange(len(df.condition)),np.zeros(len(df.condition),),df.condition]).transpose()\n",
    "    events = events.astype('int')\n",
    "    epochs = mne.EpochsArray(data, info=epochs.info, events=events, tmin = epochs.tmin)\n",
    "    epochs.apply_baseline()\n",
    "    '''\n",
    "    if kind == 'perc':\n",
    "        epochs.metadata = df[['block','condition','orientation','origin']]\n",
    "    else:\n",
    "        epochs.metadata = df[['block','condition']]\n",
    "        '''\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_eeg(fname, event_ids, events=None, segment_times=(-0.1,1), \n",
    "                      filt=(0.1,50), crop=None, resample = False):\n",
    "    ''' Preprocesses EEG data and return segmented epoch structure and events matrix \n",
    "        and indices for eeg channels.\n",
    "        Performs: montage, filter, segmentation, baseline correction, events extraction.\n",
    "        Optional: cropping, resampling\n",
    "        \n",
    "    Args\n",
    "    :param fname: address + file name\n",
    "    :param event_ids: the triggers around which to segemnt\n",
    "    \n",
    "    Optional args: \n",
    "    :param events: customary event matrix (3 columns)\n",
    "    :param segment_times: customary time points for segmentation (default: -0.1 to 1)\n",
    "    :param filt: customary filter values (default: Butterworth with 0.1, \n",
    "                    50 Hz bandpass and order 4)\n",
    "    :param crop: cropping times in seconds\n",
    "    :param resample: desired sampling rate\n",
    "    \n",
    "    Returns:\n",
    "    :return epochs: MNE structure with individual epochs\n",
    "    '''\n",
    "    \n",
    "    import mne\n",
    "    import numpy as np\n",
    "    import os.path as op\n",
    "\n",
    "    montage = mne.channels.read_montage(\"standard_1020\")\n",
    "    raw = mne.io.read_raw_bdf(fname,montage=montage,\n",
    "                              preload=True).filter(filt[0], filt[1], method='iir')\n",
    "    \n",
    "    # Notch filter if needed\n",
    "    if filt[1]>60:\n",
    "        raw.notch_filter(np.arange(60, filt[1], 60), fir_design='firwin')\n",
    "        \n",
    "    \n",
    "    # Cropping\n",
    "    if crop is not None:\n",
    "        raw.crop(crop[0], crop[1])\n",
    "    \n",
    "    # Segmenting\n",
    "    if events is None:\n",
    "        events = mne.find_events(raw, initial_event=True, \n",
    "                                 consecutive=True, shortest_event=1)\n",
    "        \n",
    "    baseline = (None, 0)  # means from the first instant to t = 0     \n",
    "    epochs = mne.Epochs(raw, events, event_ids, segment_times[0], segment_times[1],\n",
    "                    baseline=baseline, preload=True)\n",
    "    \n",
    "    # Resampling\n",
    "    if resample:\n",
    "        epochs = epochs.resample(resample)\n",
    "        \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eeg_svm(X, Y, cv, aver_n_trials=False, n_pca=False, fft=False):\n",
    "    ''' Running SVM  \n",
    "    :param X: 2D data\n",
    "    :param Y: targets (length should be as 0th dimension of X)\n",
    "    :param aver_n_trials: number of trials to average over. Default=False\n",
    "                            no average\n",
    "    :param n_pca: number of principle component to retain\n",
    "    :return confusion: confusibility matrix for all available classes\n",
    "    :return: duration: time that it took to run the procedure\n",
    "    \n",
    "    >>> confusion,duration=run_eeg_svm(X,Y,cv=10,aver_n_trials=4,n_pca=100)\n",
    "    '''\n",
    "    \n",
    "    import time\n",
    "    import random\n",
    "    random.seed()\n",
    "    from sklearn.preprocessing import scale\n",
    "    #from sklearn import svm\n",
    "    #from sklearn.svm import LinearSVC\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    import itertools\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    import numpy as np\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    t=time.time()\n",
    "    dataIn = scale(X)\n",
    "    if n_pca:\n",
    "        pca = PCA(n_components = n_pca)\n",
    "        dataIn = pca.fit_transform(dataIn)\n",
    "    else:\n",
    "        print('No PCA')\n",
    "        \n",
    "    if fft:\n",
    "        dataIn = np.abs(np.fft.fft(dataIn))\n",
    "    else:\n",
    "        print('No fft')\n",
    "        \n",
    "    labelsIn = Y\n",
    "    results = list()\n",
    "    #clf = svm.SVC(decision_function_shape='ovo', verbose=0)\n",
    "    #clf = LinearSVC(C=0.1)\n",
    "    from sklearn.svm import SVC\n",
    "    clf = SVC(kernel='linear', C=0.1)\n",
    "    nums = len(list(itertools.combinations(np.unique(labelsIn), 2)))\n",
    "    for idx, i in enumerate(itertools.combinations(np.unique(labelsIn), 2)):\n",
    "        #print(i)\n",
    "        X = dataIn[np.logical_or(labelsIn==i[0], labelsIn==i[1]),:]\n",
    "        Y = labelsIn[np.logical_or(labelsIn==i[0], labelsIn==i[1])]\n",
    "        if aver_n_trials:\n",
    "            X,Y=aver_trials_2D_mat(X,Y,aver_n_trials)\n",
    "        scores = cross_val_score(clf, X, Y, cv=cv, scoring='accuracy')\n",
    "        results.append(scores.mean())\n",
    "        #update_progress(idx / nums)\n",
    "    \n",
    "    #update_progress(1)\n",
    "    confusion = squareform(np.array(results))\n",
    "    duration=time.time() - t\n",
    "    duration=duration/60\n",
    "    print(f'The overall upright accuracy is {np.mean(squareform(confusion[:50,:50]))*100:.1f} and' +\n",
    "          f' the duration is {duration:.1f} minutes')\n",
    "    return (confusion, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_epochs_to_2D_array(epochs, times=(0.05,0.65), dims=False, threshold=3,pick_ch=[]):\n",
    "    ''' Extracts data from the MNE epoch structure, zscores, thresholds and \n",
    "        converts into 2D array.\n",
    "    :param epochs: MNE epochs structure\n",
    "    :prams times: the begining and the end time within the epoch to extract (default: 50 to 650)\n",
    "    :param dims: list of dimensions acorss which to z-transform. Default - None.\n",
    "    :param threshold: threshold value. Default - None\n",
    "    :return max: - zscored and thresholded array with the original nd_array dimensions\n",
    "    \n",
    "    >>> X, Y = convert_epochs_to_2D_array(epochs_02, times=(0.05,0.65), dims=[1, 2])\n",
    "    '''\n",
    "    \n",
    "    import mne\n",
    "    import numpy as np\n",
    "    picks = mne.pick_types(epochs.info, eeg=True)\n",
    "    Y = epochs.events[:, 2]\n",
    "    pick_ch = mne.pick_channels(ch_names=epochs.info['ch_names'], include=pick_ch)\n",
    "    X = epochs.copy().crop(times[0], times[1]).get_data()[:,pick_ch,:]\n",
    "    if dims:\n",
    "        X = zscore_threshold(X, dims, threshold)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1]*X.shape[2]), order='F')\n",
    "    return (X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    ''' Creates a text progress bar and prints out iteration identity\n",
    "    :param: progress - total number of iterations\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    import time, sys\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aver_trials_2D_mat(X,Y,n_trials):\n",
    "    '''Averages n individual trials and updates the traget vector\n",
    "    :param X: 2D numpy array\n",
    "    :param Y: 1D target vector\n",
    "    :param n_trials: number of trials (rows) to average over\n",
    "    \n",
    "    >>> X,Y=aver_trials(X,Y,3)\n",
    "    '''\n",
    "    \n",
    "    assert np.unique(Y).shape[0]==2\n",
    "    class_1,class_2 = np.unique(Y)\n",
    "    \n",
    "    \n",
    "    def comp_mat(X,Y,class_val, n_trials):\n",
    "        X = X[Y==class_val, :]\n",
    "        Y = Y[Y==class_val]\n",
    "        if len(X)%n_trials:\n",
    "            # padding the matrix if not enough trials\n",
    "            X = np.pad(X.astype(float), ((0,n_trials-len(X)%n_trials), (0,0)), \n",
    "                      'constant', constant_values=(np.nan,))\n",
    "        X=np.reshape(X,(n_trials,int(len(Y)/n_trials),-1), order='F')\n",
    "        return np.squeeze(np.nanmean(X, axis=0))\n",
    "    X_1 = comp_mat(X,Y,class_1, n_trials)\n",
    "    Y_1 = np.ones((X_1.shape[0],), dtype = 'int' )*class_1\n",
    "    X_2 = comp_mat(X,Y,class_2, n_trials)\n",
    "    Y_2 = np.ones((X_2.shape[0],), dtype = 'int' )*class_2\n",
    "    return np.concatenate((X_1, X_2)),np.concatenate((Y_1, Y_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_events_perception(evs):\n",
    "    '''Creates metadata for events\n",
    "    :param evs: third column from the event array\n",
    "    :return meta_events: meta events pandas data frame\n",
    "    \n",
    "    >>> meta_events=create_meta_events(epochs.events[:,2])\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    meta_events=pd.DataFrame({'ids': evs.astype('int')})\n",
    "    meta_events['block'] = np.nan\n",
    "    meta_events['orientation'] = np.nan\n",
    "    meta_events['origin'] = np.nan\n",
    "    \n",
    "    # creating dictionary with triggers\n",
    "    trigs=list(range(101, 151))+list(range(201, 251))\n",
    "    dicts={str(x): [1, 1] for x in trigs}\n",
    "    \n",
    "    meta_events['orientation']=np.where(meta_events['ids']>200,'in','up')\n",
    "    east_inds=np.concatenate((np.arange(101,126),np.arange(201,226)))\n",
    "    meta_events['origin']=np.where(meta_events['ids'].isin(east_inds),'east','west')\n",
    "    for index, row in meta_events.iterrows():\n",
    "        # block number\n",
    "        val=str(int(row['ids']))\n",
    "        if dicts[val][0] < 5:\n",
    "            meta_events.loc[index, ['block']] = int(dicts[val][1])\n",
    "            dicts[val][0] += 1\n",
    "        else:\n",
    "            dicts[val][0] = 2\n",
    "            dicts[val][1] += 1\n",
    "            meta_events.loc[index, ['block']] = int(dicts[val][1])\n",
    "    return meta_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pandas(epochs, num_trials, num_blocks, items_n, average=False, \n",
    "                      zscore=False, threshold=False):\n",
    "    ''' Converts an epcoh array to pandas with times and electrodes as features,\n",
    "        Optonally Z-scores within time X els, replaces threshshold (if provided) \n",
    "        and averages trials within blocks. \n",
    "    :param epochs: epochs structure\n",
    "    :param num_trials: number of trials to average\n",
    "    :param num_blocks: number of blochs in the data\n",
    "    :param items_n: number of unique triggers\n",
    "    :param average: Average in blocks. Default - False\n",
    "    :param zscore: to zscore along time X electrodes, Default - False\n",
    "    :param threshold: the threshold absolute z-value. Everything above is threshold, \n",
    "                        under is -threshold\n",
    "    :return df: - Pandas Data Frame \n",
    "    \n",
    "    >>> df = convert_to_pandas(epochs, 4, 11,\n",
    "                                average = False, zscore = True, threshold = 3)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    import mne\n",
    "    import numpy as np\n",
    "    \n",
    "    df = epochs.to_data_frame()\n",
    "    df = df.unstack(level = -1)\n",
    "    df['block']=np.tile(np.arange(1,num_blocks+1).repeat(num_trials),items_n)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['condition']=df['condition'].apply(pd.to_numeric)\n",
    "    east=list(np.arange(101,126))+list(np.arange(201,226))\n",
    "    if zscore:\n",
    "        arr = df.iloc[:,2:-1].values\n",
    "        arr = stats.zscore(arr,axis=1)\n",
    "        if threshold:\n",
    "            arr[np.where(arr>threshold)]=threshold\n",
    "            arr[np.where(arr<-threshold)]=-threshold\n",
    "        df.iloc[:,2:-1]=arr\n",
    "    if average:\n",
    "        df=df.groupby(['block','condition']).mean()\n",
    "        df.reset_index(inplace=True)\n",
    "    df['orientation']=np.where(df['condition']>200,'inv','up')\n",
    "    df['origin']=np.where(df['condition'].isin(east),'east','west')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_threshold_epochs(epochs, dims, threshold):\n",
    "    ''' Returns a matrix with zscored and thresholded values along \n",
    "        the specified directions.\n",
    "    Args:\n",
    "    :param epochs: epochs MNE array\n",
    "    :param dim: dimensions to z-transform over as list or int\n",
    "    :param threshold: threshold value\n",
    "    Return:\n",
    "    :return mat: - zscored and thresholded epochs mne structure\n",
    "    \n",
    "    >>> epochs_02 = zscore_threshold_epochs(epochs_02, dims=2, threshold=3) \n",
    "    '''\n",
    "    import mne\n",
    "    try:\n",
    "        temp = epochs.metadata\n",
    "    except: \n",
    "        print('No metadata is found')\n",
    "        \n",
    "    data = epochs.get_data()\n",
    "    data = zscore_threshold(data, dims, threshold)\n",
    "    epochs = mne.EpochsArray(data, epochs.info, events=epochs.events, tmin = epochs.tmin)\n",
    "    epochs.metadata = temp\n",
    "        \n",
    "    # Initialize an info structure\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_df(folder, extension = 'tif'):\n",
    "    '''Loads all images in the folder, removes backgrounds and returns a \n",
    "         pandas data frame with column 'names' as file names\n",
    "    Args: \n",
    "    :param folder: the folder with images\n",
    "    :param extension: image extension. Default - tif\n",
    "    '''\n",
    "    \n",
    "    import os\n",
    "    import glob\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    # definig the folder\n",
    "    os.chdir(folder)\n",
    "    # loading images\n",
    "    image_list = []\n",
    "    flList=list(glob.glob('*.'+extension))\n",
    "    flList.sort()\n",
    "    for filename in flList: \n",
    "        print(filename)\n",
    "        print(len(image_list))\n",
    "        img = Image.open(filename)\n",
    "        img = np.array(img)\n",
    "        img.shape\n",
    "        print(type(img))\n",
    "        print('c')\n",
    "        image_list.append(img)     \n",
    "    data=[]\n",
    "    [data.append(np.array(np.asarray(i)).ravel()) for i in image_list]\n",
    "    data=np.array(data)\n",
    "    mask=np.sum(data,0)==0\n",
    "    data=data[:,np.logical_not(mask)]\n",
    "    feature_num=data.shape[1]\n",
    "    df=pd.DataFrame(data)\n",
    "    df['names']=flList\n",
    "    df.names = df.names.str[:-6]\n",
    "    os.chdir(cwd)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_observer_confusibility_compute(df, feature_num = -1):\n",
    "    '''Computes theoretical observer confusibility matrix\n",
    "    Args:\n",
    "    :param df: data frame with pixels of each identity as rows\n",
    "    :param feature_num: number of features where pixels are stored \n",
    "    starting from the first column\n",
    "    :return conf_mat: confusibility matrix\n",
    "    '''\n",
    "    import itertools\n",
    "    from scipy.spatial import distance\n",
    "    data=np.array(df.iloc[:,:feature_num])\n",
    "    conf_mat=np.zeros([data.shape[0],data.shape[0]])\n",
    "    all_combos = list(itertools.combinations(range(df.shape[0]), 2))\n",
    "    for i in range(len(all_combos)):\n",
    "        conf_mat[i]=distance.euclidean(data[i[0],:], data[i[1],:])\n",
    "        conf_mat[i[1],i[0]]=distance.euclidean(data[i[1],:], data[i[0],:])\n",
    "    conf_mat=conf_mat/max(conf_mat.ravel())\n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm_coef(X, Y, aver_n_trials=False, n_pca=False, fft=False):\n",
    "    ''' Fitting SVM and returning coefficients.\n",
    "    :param X: 2D data\n",
    "    :param Y: targets (length should be as 0th dimension of X)\n",
    "    :param aver_n_trials: number of trials to average over. Default=False\n",
    "                            no average\n",
    "    :param n_pca: number of principle component to retain\n",
    "    :return results: matrix (classification pairs X 64)\n",
    "    :return: duration: time that it took to run the procedure\n",
    "    \n",
    "    >>> results,duration=run_eeg_svm(X,Y,cv=10,aver_n_trials=4,n_pca=100)\n",
    "    '''\n",
    "    \n",
    "    import time\n",
    "    from sklearn.preprocessing import scale\n",
    "    #from sklearn import svm\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    import itertools\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    import numpy as np\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    t=time.time()\n",
    "    dataIn = scale(X)\n",
    "    if n_pca:\n",
    "        pca = PCA(n_components = n_pca)\n",
    "        dataIn = pca.fit_transform(dataIn)\n",
    "    else:\n",
    "        print('No PCA')\n",
    "        \n",
    "    if fft:\n",
    "        dataIn = np.abs(np.fft.fft(dataIn))\n",
    "    else:\n",
    "        print('No fft')\n",
    "        \n",
    "    labelsIn = Y\n",
    "    results = list()\n",
    "    #clf = svm.SVC(decision_function_shape='ovo', verbose=0)\n",
    "    clf = LinearSVC(C=1.0)\n",
    "    nums = len(list(itertools.combinations(np.unique(labelsIn), 2)))\n",
    "    for idx, i in enumerate(itertools.combinations(np.unique(labelsIn), 2)):\n",
    "        print(i)\n",
    "        X = dataIn[np.logical_or(labelsIn==i[0], labelsIn==i[1]),:]\n",
    "        Y = labelsIn[np.logical_or(labelsIn==i[0], labelsIn==i[1])]\n",
    "        if aver_n_trials:\n",
    "            X,Y=aver_trials_2D_mat(X,Y,aver_n_trials)\n",
    "        clf.fit(X, Y)\n",
    "        temp=clf.coef_\n",
    "        temp=np.reshape(temp,(64,-1),order='F')\n",
    "        temp=np.mean(temp, axis=1)\n",
    "        results.append(temp)\n",
    "        update_progress(idx / nums)\n",
    "    \n",
    "    update_progress(1)\n",
    "    duration=time.time() - t\n",
    "    duration=duration/60\n",
    "    print(f' The duration is {duration:.1f} minutes')\n",
    "    return (results, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coef_by_cond(epochs_perc, times=(0.05,0.65)):\n",
    "    # features for perception map\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import mne\n",
    "    \n",
    "    X, Y = convert_epochs_to_2D_array(epochs_perc[[str(x) for x in range(101,126)]], times=times)\n",
    "    coef, duration = run_svm_coef(X,Y)\n",
    "    coefs_1=np.mean(np.abs(np.array(coef)),0)\n",
    "    X, Y = convert_epochs_to_2D_array(epochs_perc[[str(x) for x in range(126,151)]], times=times)\n",
    "    coef, duration = run_svm_coef(X,Y)\n",
    "    coefs_2=np.mean(np.abs(np.array(coef)),0)\n",
    "    X, Y = convert_epochs_to_2D_array(epochs_perc[[str(x) for x in range(201,226)]], times=times)\n",
    "    coef, duration = run_svm_coef(X,Y)\n",
    "    coefs_3=np.mean(np.abs(np.array(coef)),0)\n",
    "    X, Y = convert_epochs_to_2D_array(epochs_perc[[str(x) for x in range(226,251)]], times=times)\n",
    "    coef, duration = run_svm_coef(X,Y)\n",
    "    coefs_4=np.mean(np.abs(np.array(coef)),0)\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 10),nrows=2,ncols=2)\n",
    "    mne.viz.plot_topomap(coefs_1,epochs_perc.info,vmin=np.min(coefs_1),axes=ax[0,0], show=False)\n",
    "    ax[0,0].set_title('upright unfamiliar', fontsize=14)\n",
    "    mne.viz.plot_topomap(coefs_2,epochs_perc.info,vmin=np.min(coefs_2),axes=ax[0,1], show=False)\n",
    "    ax[0,1].set_title('upright famous', fontsize=14)\n",
    "    mne.viz.plot_topomap(coefs_3,epochs_perc.info,vmin=np.min(coefs_3),axes=ax[1,0], show=False)\n",
    "    ax[1,0].set_title('inverted unfamiliar', fontsize=14)\n",
    "    mne.viz.plot_topomap(coefs_4,epochs_perc.info,vmin=np.min(coefs_4),axes=ax[1,1], show=False)\n",
    "    ax[1,1].set_title('inverted famous', fontsize=14)\n",
    "    plt.show()\n",
    "    return [coefs_1, coefs_2, coefs_3, coefs_4]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coef_all(epochs_perc, times=(0.05,0.65)):\n",
    "    # features for perception map\n",
    "    import matplotlib.pyplot as plt\n",
    "    import mne\n",
    "    import numpy as np\n",
    "    \n",
    "    X, Y = convert_epochs_to_2D_array(epochs_perc, times=times)\n",
    "    coef, duration = run_svm_coef(X,Y)\n",
    "    coefs=np.mean(np.abs(np.array(coef)),0)\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    mne.viz.plot_topomap(coefs,epochs_perc.info,vmin=np.min(coefs), axes=ax)\n",
    "    ax.set_title('across conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lbl(listIn):\n",
    "    ''' Converts labels to 4 (1-upright unfamiliar, 2-upright famous, \n",
    "        3 - inverted unfamiliar, 4 - inverted famous)\n",
    "    :param: numpy 1D array with labels\n",
    "    :return: numpy 1D array with labels\n",
    "    '''\n",
    "    up_un=np.arange(101,126)\n",
    "    up_fa=np.arange(126,151)\n",
    "    in_un=np.arange(201,226)\n",
    "    in_fa=np.arange(226,251)\n",
    "    list_out=[]\n",
    "    for i in listIn:\n",
    "        if i in up_un:\n",
    "            list_out.append(1)\n",
    "        elif i in up_fa:\n",
    "            list_out.append(2)\n",
    "        elif i in in_un:\n",
    "            list_out.append(3)\n",
    "        elif i in in_fa:\n",
    "            list_out.append(4)\n",
    "        else:\n",
    "            error\n",
    "    return list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiple_conf_mats(perc_file, imag_file):\n",
    "    perc_list=[]\n",
    "    imag_list=[]\n",
    "    for i,j in zip(perc_file,imag_file):\n",
    "        perc_list.append(pd.read_csv(i,header=None))\n",
    "        imag_list.append(pd.read_csv(j, index_col = 0))\n",
    "    return(perc_list,imag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_sub_corr(df_perc, df_imag):\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    %run general_tools.ipynb\n",
    "    \n",
    "    im_names = ['mcy','sgo','sjo','est','tsw']\n",
    "    corr_names = ['est','mcy','sjo','sgo','tsw']\n",
    "    pr_names = ['adi','ani','ama','ago','aza','ekl','evu','epo','eiv','ech','ian','jpi','kda',\n",
    "               'kgo','mbo','mbe','ofa','pan','pga','rga','siv','tar','tka','yst','ype','ase',\n",
    "               'aha','ake','cmo','eol','epa','ecl','ero','est','ewa','jla','jal','kpe','kkn',\n",
    "                'kst','mcy','ndo','npo','owi','pcr','rmc','rwi','sjo','sgo','tsw']\n",
    "    \n",
    "    # Theoretical observer:\n",
    "    #folder1 = 'C:/Users/danne/Documents/UofT/FamousRecon/set3/east'\n",
    "    #folder2 = 'C:/Users/danne/Documents/UofT/FamousRecon/set3/west'\n",
    "    folder1 = 'C:\\\\Users\\\\nemrodov\\\\Documents\\\\Ilya_study\\\\builder_exp\\\\set3\\\\east'\n",
    "    folder2 = 'C:\\\\Users\\\\nemrodov\\\\Documents\\\\Ilya_study\\\\builder_exp\\\\set3\\\\west'\n",
    "    df = pd.concat((image_to_df(folder1),image_to_df(folder2)))\n",
    "    df['origin'] = ['east']*25+['west']*25\n",
    "    conf = squareform(pdist(df.iloc[:,:16428],'euclidean'))\n",
    "    df_to = pd.DataFrame(conf, columns = pr_names, index = pr_names)\n",
    "\n",
    "    df_perc.columns = [pr_names*2]\n",
    "    df_perc.index = [pr_names*2]\n",
    "    idx = pd.IndexSlice\n",
    "    df_perc_imag = df_perc.loc[idx[im_names], idx[im_names]]\n",
    "    df_imag = df_imag.reindex(corr_names)\n",
    "    df_imag = df_imag.transpose().reindex(corr_names).transpose()\n",
    "    df_to_imag = df_to.loc[idx[im_names], idx[im_names]]\n",
    "    df_to_imag = df_to_imag.reindex(corr_names)\n",
    "    df_to_imag = df_to_imag.transpose().reindex(corr_names).transpose()\n",
    "  \n",
    "    # compute correlations\n",
    "    up_corr = np.corrcoef(squareform(df_imag),squareform(df_perc_imag.iloc[:5,:5].values))\n",
    "    print(f'Correlation between upright perceived and imagery discrimination is {up_corr[1,0]:.2f}')\n",
    "    in_corr = np.corrcoef(squareform(df_imag),squareform(df_perc_imag.iloc[-5:,-5:].values))\n",
    "    print(f'Correlation between inverted perceived and imagery discriminations is {in_corr[1,0]:.2f}')\n",
    "    to_imag_corr = np.corrcoef(squareform(df_to_imag),squareform(df_imag.iloc[:5,:5].values))\n",
    "    print(f'Correlation between imagery and TO {to_imag_corr[1,0]:.2f}')\n",
    "\n",
    "\n",
    "    up_perc_to_corr = np.corrcoef(squareform(df_to.values),squareform(df_perc.iloc[:50,:50].values))\n",
    "    print(f'Correlation between upright perceived and TO discriminations is {up_perc_to_corr[1,0]:.2f}')\n",
    "    in_perc_to_corr = np.corrcoef(squareform(df_to.values),squareform(df_perc.iloc[50:100,50:100].values))\n",
    "    print(f'Correlation between inverted perceived and TO discriminations is {in_perc_to_corr[1,0]:.2f}')\n",
    "    up_perc_to_corr_unf = np.corrcoef(squareform(df_to.values[:25,:25]),squareform(df_perc.iloc[:25,:25].values))\n",
    "    print(f'Correlation between unfamiliar upright perceived and TO discriminations is {up_perc_to_corr_unf[1,0]:.2f}')\n",
    "    up_perc_to_corr_fam = np.corrcoef(squareform(df_to.values[25:50,25:50]),squareform(df_perc.iloc[25:50,25:50].values))\n",
    "    print(f'Correlation between famous upright perceived and TO discriminations is {up_perc_to_corr_fam[1,0]:.2f}')\n",
    "\n",
    "    in_perc_to_corr_unf = np.corrcoef(squareform(df_to.values[:25,:25]),squareform(df_perc.iloc[50:75,50:75].values))\n",
    "    print(f'Correlation between unfamiliar inverted perceived and TO discriminations is {in_perc_to_corr_unf[1,0]:.2f}')\n",
    "    in_perc_to_corr_fam = np.corrcoef(squareform(df_to.values[25:50,25:50]),squareform(df_perc.iloc[75:100,75:100].values))\n",
    "    print(f'Correlation between famous inverted perceived and TO discriminations is {in_perc_to_corr_fam[1,0]:.2f}')\n",
    "    return (df_perc_imag, df_imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def presenting_results(results, to_print = False, ind = False):\n",
    "    ''' Aggregates individual results into a panda data frame.\n",
    "    :param results: list with confusibility matrices\n",
    "    :return out: - Pandas Data Frame \n",
    "    \n",
    "    >>> df = presenting results(epochs, to_print = True)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    from scipy.spatial.distance import squareform\n",
    "    data = {'up unf' : [np.mean(squareform(i[0:25,0:25]))*100 for i in results],\n",
    "           'up fam' : [np.mean(squareform(i[25:50,25:50]))*100 for i in results],\n",
    "           'inv_unf' : [np.mean(squareform(i[50:75,50:75]))*100 for i in results],\n",
    "           'inv fam' : [np.mean(squareform(i[75:100,75:100]))*100 for i in results],\n",
    "           'up' : [np.mean(squareform(i[0:50,0:50]))*100 for i in results],\n",
    "           'inv' : [np.mean(squareform(i[50:100,50:100]))*100 for i in results],\n",
    "           'all' : [np.mean(squareform(i))*100 for i in results]}\n",
    "    out = pd.DataFrame.from_dict(data)\n",
    "    if ind:\n",
    "        out['Subs']=ind\n",
    "        out.set_index('Subs')\n",
    "    if to_print:\n",
    "        print(out)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_epochs_to_3D_array(epochs, times=(0.05,0.65), pick_ch=[]):\n",
    "    ''' Extracts data from the MNE epoch structure, zscores, thresholds and \n",
    "        converts into 3D array.\n",
    "    :param epochs: MNE epochs structure\n",
    "    :prams times: the begining and the end time within the epoch to extract (default: 50 to 650)\n",
    "    :return X: - 3D data array\n",
    "    :return Y: - 1D vector of labels\n",
    "    \n",
    "    >>> X, Y = convert_epochs_to_2D_array(epochs_02, times=(0.05,0.65))\n",
    "    '''\n",
    "    \n",
    "    import mne\n",
    "    import numpy as np\n",
    "    picks = mne.pick_types(epochs.info, eeg=True)\n",
    "    Y = epochs.events[:, 2]\n",
    "    pick_ch = mne.pick_channels(ch_names=epochs.info['ch_names'], include=pick_ch)\n",
    "    X = epochs.copy().crop(times[0], times[1]).get_data()[:,pick_ch,:]\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_perc_as_imag(df_perc, df_imag):\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    %run general_tools.ipynb\n",
    "    \n",
    "    im_names = ['mcy','sgo','sjo','est','tsw']\n",
    "    corr_names = ['est','mcy','sjo','sgo','tsw']\n",
    "    pr_names = ['adi','ani','ama','ago','aza','ekl','evu','epo','eiv','ech','ian','jpi','kda',\n",
    "               'kgo','mbo','mbe','ofa','pan','pga','rga','siv','tar','tka','yst','ype','ase',\n",
    "               'aha','ake','cmo','eol','epa','ecl','ero','est','ewa','jla','jal','kpe','kkn',\n",
    "                'kst','mcy','ndo','npo','owi','pcr','rmc','rwi','sjo','sgo','tsw']\n",
    " \n",
    "\n",
    "    df_perc.columns = [pr_names*2]\n",
    "    df_perc.index = [pr_names*2]\n",
    "    idx = pd.IndexSlice\n",
    "    df_perc_imag = df_perc.loc[idx[im_names], idx[im_names]]\n",
    "    df_imag = df_imag.reindex(corr_names)\n",
    "    df_imag = df_imag.transpose().reindex(corr_names).transpose()\n",
    "\n",
    "    return (df_perc_imag, df_imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_perc_5(df_perc):\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    import pandas as pd\n",
    "    df_perc = pd.DataFrame(df_perc)\n",
    "    %run general_tools.ipynb\n",
    "    \n",
    "    im_names = ['mcy','sgo','sjo','est','tsw']\n",
    "    corr_names = ['est','mcy','sjo','sgo','tsw']\n",
    "    pr_names = ['adi','ani','ama','ago','aza','ekl','evu','epo','eiv','ech','ian','jpi','kda',\n",
    "               'kgo','mbo','mbe','ofa','pan','pga','rga','siv','tar','tka','yst','ype','ase',\n",
    "               'aha','ake','cmo','eol','epa','ecl','ero','est','ewa','jla','jal','kpe','kkn',\n",
    "                'kst','mcy','ndo','npo','owi','pcr','rmc','rwi','sjo','sgo','tsw']\n",
    " \n",
    "\n",
    "    df_perc.columns = [pr_names]\n",
    "    df_perc.index = [pr_names]\n",
    "    idx = pd.IndexSlice\n",
    "    df_perc_imag = df_perc.loc[idx[im_names], idx[im_names]]\n",
    "    \n",
    "\n",
    "    return (df_perc_imag.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_epochs(fnames, event_ids, im_times, filt):\n",
    "    import mne\n",
    "    import numpy as np\n",
    "    import os.path as op\n",
    "    %run general_tools.ipynb\n",
    "    infolder, outfolder = find_folder()\n",
    "    baseline = (None, 0)\n",
    "    \n",
    "    montage = mne.channels.read_montage(\"standard_1020\")\n",
    "    raw = []\n",
    "    for fname in fnames:\n",
    "        fname = op.join(infolder,fname)  \n",
    "        raw.append(mne.io.read_raw_bdf(fname,montage=montage, \n",
    "                                       preload=True).filter(filt[0], filt[1], method='iir'))\n",
    "    raw = mne.concatenate_raws(raw)\n",
    "    events = mne.find_events(raw, initial_event=True, \n",
    "                                     consecutive=True, shortest_event=1)\n",
    "    epochs = mne.Epochs(raw, events, event_ids, im_times[0], im_times[1],\n",
    "                    baseline=baseline, preload=True)\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_epochs_perc(fnames, event_ids, im_times, filt):\n",
    "    import mne\n",
    "    import numpy as np\n",
    "    import os.path as op\n",
    "    from mne.channels import make_standard_montage\n",
    "    %run general_tools.ipynb\n",
    "    infolder, outfolder = find_folder()\n",
    "    baseline = (None, 0)\n",
    "    \n",
    "   \n",
    "    montage = make_standard_montage('biosemi64')\n",
    "    \n",
    "    epochs = []\n",
    "    for fname in fnames:\n",
    "        fname = op.join(infolder,fname) \n",
    "        raw = mne.io.read_raw_bdf(fname, preload=True).filter(filt[0], filt[1], method='iir')\n",
    "        raw.set_montage(montage)\n",
    "        events = mne.find_events(raw, initial_event=True, \n",
    "                                 consecutive=True, shortest_event=1, verbose=0)\n",
    "        temp = mne.Epochs(raw, events, event_ids, im_times[0], im_times[1],\n",
    "                          baseline=baseline, preload=True, detrend = 1)\n",
    "        temp = temp[100:]\n",
    "        epochs.append(temp)\n",
    "\n",
    "    epochs = mne.concatenate_epochs(epochs)\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_imag(sub, filt, resample, im_times, dims, trials = 8, blocks = 12):\n",
    "    \n",
    "    ''' Running preprocssing on all imagery data and saving the results in fif format\n",
    "    :param sub: subject number\n",
    "    :param filt: tuple with low and high cuts\n",
    "    :param im_times: time limits for the epochs\n",
    "    :param trials: number of trials to average in a single block\n",
    "    :param blocks: number of blocks\n",
    "    :param: dims: dimensions to score across\n",
    "    :return: by default epochs in blocks, if 'block' is zero, returns epochs with trials\n",
    "    \n",
    "    >>> av = preprocess_all_imag(4, (0.1,50), 256, (-0.1,5.6), (2,3), trials = 8, blocks = 12)\n",
    "    >>> ep = preprocess_all_imag(4, (0.1,50), 256, (-0.1,5.6), (2,3), blocks = 0)\n",
    "    '''\n",
    "    \n",
    "    import mne\n",
    "    import numpy as np\n",
    "    import os.path as op\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    %run EEG_auxiliary_module.ipynb\n",
    "    %run general_tools.ipynb\n",
    "    mne.set_log_level(\"WARNING\")\n",
    "\n",
    "    infolder, outfolder = find_folder()\n",
    "    \n",
    "    sub = str(sub).zfill(2)\n",
    "\n",
    "\n",
    "    # Imagery definitions\n",
    "    event_ids={str(x):x for x in list(range(31, 36))}\n",
    "    fnames = ['IR_'+sub+'_S01.bdf','IR_'+sub+'_S02.bdf']\n",
    "\n",
    "\n",
    "    # Loading imagery epochs\n",
    "    epochs_im = load_to_epochs(fnames, event_ids, im_times, filt)\n",
    "    epochs_im.drop_channels(['Status']).equalize_event_counts(event_ids=event_ids, method='mintime') \n",
    "    epochs_im = zscore_threshold_epochs(epochs_im, dims, 3) \n",
    "    epochs_im.set_eeg_reference(\"average\")\n",
    "    fname = op.join(outfolder,'S'+sub+'_imag-epo.fif')\n",
    "    epochs_im.save(fname)\n",
    "    \n",
    "    if  blocks != 0:\n",
    "        # averaging by block\n",
    "        epochs_im = block_average(epochs_im, trials, blocks, kind = 'imagery')\n",
    "        epochs_im.resample(resample)\n",
    "        fname = op.join(outfolder,'S'+sub+'_imag_aver-epo.fif')\n",
    "        epochs_im.save(fname)\n",
    "    \n",
    "    return epochs_im\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_perc(sub, filt, resample, im_times, dims, trials = 8, blocks = 12):\n",
    "    \n",
    "    ''' Running preprocssing on all imagery data and saving the results in fif format\n",
    "    :param sub: subject number\n",
    "    :param filt: tuple with low and high cuts\n",
    "    :param im_times: time limits for the epochs\n",
    "    :param trials: number of trials to average in a single block\n",
    "    :param blocks: number of blocks\n",
    "    :param: dims: dimensions to score across\n",
    "    :return: by default epochs in blocks, if 'block' is zero, returns epochs with trials\n",
    "    \n",
    "    >>> av = preprocess_all_imag(4, (0.1,50), 256, (-0.1,5.6), (2,3), trials = 8, blocks = 12)\n",
    "    >>> ep = preprocess_all_imag(4, (0.1,50), 256, (-0.1,5.6), (2,3), blocks = 0)\n",
    "    '''\n",
    "    \n",
    "    import mne\n",
    "    import numpy as np\n",
    "    import os.path as op\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    %run EEG_auxiliary_module.ipynb\n",
    "    %run general_tools.ipynb\n",
    "    mne.set_log_level(\"WARNING\")\n",
    "\n",
    "    infolder, outfolder = find_folder()\n",
    "    \n",
    "    sub = str(sub).zfill(2)\n",
    "\n",
    "\n",
    "    # Imagery definitions\n",
    "    trigs=list(range(101, 151))+list(range(201, 251))\n",
    "    event_ids={str(x):x for x in trigs}\n",
    "    fnames = ['IR_'+sub+'_S01.bdf','IR_'+sub+'_S02.bdf']\n",
    "\n",
    "    # Loading imagery epochs\n",
    "    epochs = load_to_epochs_perc(fnames, event_ids, im_times, filt)\n",
    "    epochs.drop_channels(['Status']).equalize_event_counts(event_ids=event_ids, method='mintime') \n",
    "    epochs = zscore_threshold_epochs(epochs, dims=dims, threshold=3)\n",
    "    epochs.set_eeg_reference(\"average\")\n",
    "    fname = op.join(outfolder,'S'+sub+'_perc-epo.fif')\n",
    "    epochs.save(fname)\n",
    "    \n",
    "    if  blocks != 0:\n",
    "        # averaging by block\n",
    "        epochs = block_average(epochs, trials, blocks, kind = 'perc')\n",
    "        epochs.resample(resample)\n",
    "        fname = op.join(outfolder,'S'+sub+'_perc_aver-epo.fif')\n",
    "        epochs.save(fname)\n",
    "        \n",
    "    return epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_combs(numset,combs):\n",
    "    ''' Finding indices' pairs in vectorized confusability matrix\n",
    "    :param numset: the numbers to be found\n",
    "    :param combs: list with tuples containing all possible combinations\n",
    "    :return: list with indices corresponding to the requried pairs.\n",
    "    \n",
    "    >>> indx = find_index_combs(numset,combs)\n",
    "    '''\n",
    "    import numpy as np\n",
    "    numset = set(numset)\n",
    "    indx = []\n",
    "    for i,j in enumerate(combs):\n",
    "        if len(numset.intersection(set(j))) == 2:\n",
    "            indx.append(i)\n",
    "    return indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_names():\n",
    "    im_names = ['mcy','sgo','sjo','est','tsw']\n",
    "    corr_names = ['est','mcy','sjo','sgo','tsw']\n",
    "    pr_names = ['adi','ani','ama','ago','aza','ekl','evu','epo','eiv','ech','ian','jpi','kda',\n",
    "               'kgo','mbo','mbe','ofa','pan','pga','rga','siv','tar','tka','yst','ype','ase',\n",
    "               'aha','ake','cmo','eol','epa','ecl','ero','est','ewa','jla','jal','kpe','kkn',\n",
    "                'kst','mcy','ndo','npo','owi','pcr','rmc','rwi','sjo','sgo','tsw']\n",
    "    return (im_names, pr_names, corr_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ar(epochs, dims, threshold):\n",
    "    ''' Returns a matrix with zscored and thresholded values along \n",
    "        the specified directions.\n",
    "    Args:\n",
    "    :param epochs: epochs MNE array\n",
    "    :param dim: dimensions to z-transform over as list or int\n",
    "    :param threshold: threshold value\n",
    "    Return:\n",
    "    :return mat: - zscored and thresholded epochs mne structure\n",
    "    \n",
    "    >>> epochs_02 = zscore_threshold_epochs(epochs_02, dims=2, threshold=3) \n",
    "    '''\n",
    "    import mne\n",
    "    try:\n",
    "        temp = epochs.metadata\n",
    "    except: \n",
    "        print('No metadata is found')\n",
    "        \n",
    "    data = epochs.get_data()\n",
    "    data = zscore_threshold(data, dims, threshold)\n",
    "    epochs = mne.EpochsArray(data, epochs.info, events=epochs.events, tmin = epochs.tmin)\n",
    "    epochs.metadata = temp\n",
    "        \n",
    "    # Initialize an info structure\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product, combinations\n",
    "from scipy.special import binom\n",
    "\n",
    "class ValidationSplit():\n",
    "    def __init__(self, labels_train, labels_test):\n",
    "\n",
    "        self.labels_train = labels_train\n",
    "        self.labels_test = labels_test\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield np.arange(len(self.labels_train)), \\\n",
    "              np.arange(len(self.labels_train), len(self.labels_train) + len(self.labels_test))\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class ShuffleBinLeaveOneOut:\n",
    "\n",
    "    def __init__(self, labels, n_iter=10, n_pseudo=5):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List<int>, np.ndarray()\n",
    "            Label for each trial\n",
    "        n_iter : int\n",
    "            Number of permutations\n",
    "        n_pseudo : int\n",
    "            How many trials belong to one bin (aka pseudo-trial)\n",
    "        \"\"\"\n",
    "\n",
    "        self.labels = np.array(labels)\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.classes, self.n_trials = np.unique(labels, return_counts=True)\n",
    "        self.n_classes = self.classes.shape[0]\n",
    "        self.n_pseudo = n_pseudo\n",
    "        self._compute_pseudo_info()\n",
    "\n",
    "    def _compute_pseudo_info(self):\n",
    "        \"\"\"\n",
    "        Compute indices and labels for the pseudo-trial matrix\n",
    "        The pseudo-trial matrix is the resulting matrix *after* having grouped the data into\n",
    "        randomly permuted pseudo-trial bins and averaging trials within each bin. Thus, no\n",
    "        additional permutation is necessary for pseudo-trial indices, which are somewhat trivial.\n",
    "        \"\"\"\n",
    "        self.ind_pseudo_train = np.full((self.n_classes, self.n_classes, 2*(self.n_pseudo-1)),\n",
    "                                        np.nan, dtype=np.int)\n",
    "        self.ind_pseudo_test = np.full((self.n_classes, self.n_classes, 2), np.nan, dtype=np.int)\n",
    "        self.labels_pseudo_train = np.full((self.n_classes, self.n_classes, 2*(self.n_pseudo-1)),\n",
    "                                           np.nan, dtype=np.int)\n",
    "        self.labels_pseudo_test = np.full((self.n_classes, self.n_classes, 2), np.nan, dtype=np.int)\n",
    "        for c1 in range(self.n_classes):\n",
    "            range_c1 = range(c1*(self.n_pseudo-1), (c1+1)*(self.n_pseudo-1))\n",
    "            for c2 in range(self.n_classes):\n",
    "                range_c2 = range(c2*(self.n_pseudo-1), (c2+1)*(self.n_pseudo-1))\n",
    "                self.ind_pseudo_train[c1, c2, :2*(self.n_pseudo - 1)] = \\\n",
    "                    np.concatenate((range_c1, range_c2))\n",
    "                self.ind_pseudo_test[c1, c2] = [c1, c2]\n",
    "\n",
    "                self.labels_pseudo_train[c1, c2, :2*(self.n_pseudo - 1)] = \\\n",
    "                    np.concatenate((self.classes[c1] * np.ones(self.n_pseudo - 1),\n",
    "                                    self.classes[c2] * np.ones(self.n_pseudo - 1)))\n",
    "                self.labels_pseudo_test[c1, c2] = self.classes[[c1, c2]].astype(self.labels_pseudo_train.dtype)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Generator function for the cross-validation object. Each fold corresponds to a new random\n",
    "        grouping of trials into pseudo-trials.\n",
    "        \"\"\"\n",
    "        _ind_train = np.full(self.n_classes*(self.n_pseudo-1), np.nan, dtype=np.object)\n",
    "        _ind_test = np.full(self.n_classes, np.nan, dtype=np.object)\n",
    "        for perm in range(self.n_iter):\n",
    "            for c1 in range(self.n_classes):  # separate permutation for each class\n",
    "                prm = np.array(np.array_split(np.random.permutation(self.n_trials[c1]), self.n_pseudo))\n",
    "                ind = prm + np.sum(self.n_trials[:c1])\n",
    "                for i, j in enumerate(range(c1*(self.n_pseudo-1), (c1+1)*(self.n_pseudo-1))):\n",
    "                    _ind_train[j] = ind[i]\n",
    "                _ind_test[c1] = ind[-1]\n",
    "            yield _ind_train, _ind_test\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None):\n",
    "        return self.n_iter\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_iter\n",
    "\n",
    "class ShuffleBinLeaveOneOutWithin:\n",
    "\n",
    "    def __init__(self, labels, n_iter=10, n_pseudo=5):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List<int>, np.ndarray()\n",
    "            Label for each trial\n",
    "        n_iter : int\n",
    "            Number of permutations\n",
    "        n_pseudo : int\n",
    "            How many trials belong to one bin (aka pseudo-trial)\n",
    "        \"\"\"\n",
    "\n",
    "        self.labels = np.array(labels)\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.classes, self.n_trials = np.unique(labels, return_counts=True)\n",
    "        self.n_classes = self.classes.shape[0]\n",
    "        self.n_pseudo = n_pseudo\n",
    "        self._compute_pseudo_info()\n",
    "\n",
    "    def _compute_pseudo_info(self):\n",
    "        \"\"\"\n",
    "        Compute indices and labels for the pseudo-trial matrix\n",
    "        The pseudo-trial matrix is the resulting matrix *after* having grouped the data into\n",
    "        randomly permuted pseudo-trial bins and averaging trials within each bin. Thus, no\n",
    "        additional permutation is necessary for pseudo-trial indices, which are somewhat trivial.\n",
    "        \"\"\"\n",
    "        self.ind_pseudo_train = np.full((self.n_classes, 1, self.n_pseudo-2),\n",
    "                                        np.nan, dtype=np.int)\n",
    "        self.ind_pseudo_test = np.full((self.n_classes, 1, 2), np.nan, dtype=np.int)\n",
    "        self.labels_pseudo_train = np.full((self.n_classes, 1, self.n_pseudo-2),\n",
    "                                           np.nan, dtype=np.int)\n",
    "        self.labels_pseudo_test = np.full((self.n_classes, 1, 2), np.nan, dtype=np.int)\n",
    "        for c1 in range(self.n_classes):\n",
    "            range_c1 = range(c1*(self.n_pseudo-2), (c1+1)*(self.n_pseudo-2))\n",
    "\n",
    "            self.ind_pseudo_train[c1, 0, :self.n_pseudo - 2] = range_c1\n",
    "            self.ind_pseudo_test[c1, 0] = [c1*2, c1*2+1]\n",
    "\n",
    "            self.labels_pseudo_train[c1, 0, :self.n_pseudo - 2] = self.classes[c1] * np.ones(self.n_pseudo - 2)\n",
    "            self.labels_pseudo_test[c1, 0] = self.classes[[c1, c1]].astype(self.labels_pseudo_train.dtype)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Generator function for the cross-validation object. Each fold corresponds to a new random\n",
    "        grouping of trials into pseudo-trials.\n",
    "        \"\"\"\n",
    "        _ind_train = np.full(self.n_classes*(self.n_pseudo-2), np.nan, dtype=np.object)\n",
    "        _ind_test = np.full(self.n_classes*2, np.nan, dtype=np.object)\n",
    "        for perm in range(self.n_iter):\n",
    "            for c1 in range(self.n_classes):  # separate permutation for each class\n",
    "                prm = np.array(np.array_split(np.random.permutation(self.n_trials[c1]), self.n_pseudo))\n",
    "                ind = prm + np.sum(self.n_trials[:c1])\n",
    "                for i, j in enumerate(range(c1*(self.n_pseudo-2), (c1+1)*(self.n_pseudo-2))):\n",
    "                    _ind_train[j] = ind[i]\n",
    "                for i, j in enumerate(range(c1*2, (c1+1)*2)):\n",
    "                    _ind_test[j] = ind[-i-1]\n",
    "            yield _ind_train, _ind_test\n",
    "\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_iter\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None):\n",
    "        return self.n_iter\n",
    "\n",
    "class ShuffleBin:\n",
    "\n",
    "    def __init__(self, labels, n_iter=10, n_pseudo=5):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List<int>, np.ndarray()\n",
    "            Label for each trial\n",
    "        n_iter : int\n",
    "            Number of permutations\n",
    "        n_pseudo : int\n",
    "            How many trials belong to one bin (aka pseudo-trial)\n",
    "        \"\"\"\n",
    "\n",
    "        self.labels = np.array(labels)\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.classes, self.n_trials = np.unique(labels, return_counts=True)\n",
    "        self.n_classes = self.classes.shape[0]\n",
    "        self.n_pseudo = n_pseudo\n",
    "        self._compute_pseudo_info()\n",
    "\n",
    "    def _compute_pseudo_info(self):\n",
    "        \"\"\"\n",
    "        Compute indices and labels for the pseudo-trial matrix\n",
    "        The pseudo-trial matrix is the resulting matrix *after* having grouped the data into\n",
    "        randomly permuted pseudo-trial bins and averaging trials within each bin. Thus, no\n",
    "        additional permutation is necessary for pseudo-trial indices, which are somewhat trivial.\n",
    "        \"\"\"\n",
    "        self.ind_pseudo_test = np.full((self.n_classes, self.n_classes, 2 * self.n_pseudo),\n",
    "                                       np.nan, dtype=np.int)\n",
    "        self.labels_pseudo_test = np.full((self.n_classes, self.n_classes, 2 * self.n_pseudo),\n",
    "                                          np.nan, dtype=np.int)\n",
    "\n",
    "        for c1 in range(self.n_classes):\n",
    "            range_c1 = range(c1*self.n_pseudo, (c1+1)*self.n_pseudo)\n",
    "            for c2 in range(self.n_classes):\n",
    "                range_c2 = range(c2*self.n_pseudo, (c2+1)*self.n_pseudo)\n",
    "                self.ind_pseudo_test[c1, c2, :2 * self.n_pseudo] = np.concatenate((range_c1, range_c2))\n",
    "                self.labels_pseudo_test[c1, c2, :2 * self.n_pseudo] = \\\n",
    "                    np.concatenate((self.classes[c1] * np.ones(self.n_pseudo),\n",
    "                                    self.classes[c2] * np.ones(self.n_pseudo)))\n",
    "        self.ind_pseudo_train = []\n",
    "        self.labels_pseudo_train = []\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Generator function for the cross-validation object. Each fold corresponds to a new random\n",
    "        grouping of trials into pseudo-trials.\n",
    "        \"\"\"\n",
    "        _ind_test = np.full(self.n_classes*self.n_pseudo, np.nan, dtype=np.object)\n",
    "        _ind_train = []\n",
    "        for perm in range(self.n_iter):\n",
    "            for c1 in range(self.n_classes):  # separate permutation for each class\n",
    "                prm = np.array(np.array_split(np.random.permutation(self.n_trials[c1]), self.n_pseudo))\n",
    "                ind = prm + np.sum(self.n_trials[:c1])\n",
    "                for i, j in enumerate(range(c1*self.n_pseudo, (c1+1)*self.n_pseudo)):\n",
    "                    _ind_test[j] = ind[i]\n",
    "            yield _ind_train, _ind_test\n",
    "\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_iter\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None):\n",
    "        return self.n_iter\n",
    "\n",
    "\n",
    "class ShuffleBinTest:\n",
    "\n",
    "    def __init__(self, labels, n_iter=10, n_pseudo=5):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List<int>, np.ndarray()\n",
    "            Label for each trial\n",
    "        n_iter : int\n",
    "            Number of permutations\n",
    "        n_pseudo : int\n",
    "            How many trials belong to one bin (aka pseudo-trial)\n",
    "        \"\"\"\n",
    "\n",
    "        self.labels = np.array(labels)\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.classes, self.n_trials = np.unique(labels, return_counts=True)\n",
    "        self.n_classes = self.classes.shape[0]\n",
    "        self.n_pseudo = n_pseudo\n",
    "        self._compute_pseudo_info()\n",
    "\n",
    "    def _compute_pseudo_info(self):\n",
    "        \"\"\"\n",
    "        Compute indices and labels for the pseudo-trial matrix\n",
    "        The pseudo-trial matrix is the resulting matrix *after* having grouped the data into\n",
    "        randomly permuted pseudo-trial bins and averaging trials within each bin. Thus, no\n",
    "        additional permutation is necessary for pseudo-trial indices, which are somewhat trivial.\n",
    "        \"\"\"\n",
    "        self.ind_pseudo_test = np.full((self.n_classes, self.n_classes, 2 * self.n_pseudo),\n",
    "                                       np.nan, dtype=np.int)\n",
    "        self.labels_pseudo_test = np.full((self.n_classes, self.n_classes, 2 * self.n_pseudo),\n",
    "                                          np.nan, dtype=np.int)\n",
    "\n",
    "        for c1 in range(self.n_classes):\n",
    "            range_c1 = range(c1*self.n_pseudo, (c1+1)*self.n_pseudo)\n",
    "            for c2 in range(self.n_classes):\n",
    "                range_c2 = range(c2*self.n_pseudo, (c2+1)*self.n_pseudo)\n",
    "                self.ind_pseudo_test[c1, c2, :2 * self.n_pseudo] = np.concatenate((range_c1, range_c2))\n",
    "                self.labels_pseudo_test[c1, c2, :2 * self.n_pseudo] = \\\n",
    "                    np.concatenate((self.classes[c1] * np.ones(self.n_pseudo),\n",
    "                                    self.classes[c2] * np.ones(self.n_pseudo)))\n",
    "        self.ind_pseudo_train = []\n",
    "        self.labels_pseudo_train = []\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Generator function for the cross-validation object. Each fold corresponds to a new random\n",
    "        grouping of trials into pseudo-trials.\n",
    "        \"\"\"\n",
    "        _ind_test = np.full(self.n_classes*self.n_pseudo, np.nan, dtype=np.object)\n",
    "        _ind_train = []\n",
    "        for perm in range(self.n_iter):\n",
    "            for c1 in range(self.n_classes):  # separate permutation for each class\n",
    "                prm = np.array(np.array_split(range(self.n_trials[c1]), self.n_pseudo))\n",
    "                ind = prm + np.sum(self.n_trials[:c1])\n",
    "                for i, j in enumerate(range(c1*self.n_pseudo, (c1+1)*self.n_pseudo)):\n",
    "                    _ind_test[j] = ind[i]\n",
    "            yield _ind_train, _ind_test\n",
    "\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_iter\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.n_iter\n",
    "\n",
    "\n",
    "class ShuffleBinWithin:\n",
    "\n",
    "    def __init__(self, labels, n_iter=10, n_pseudo=5):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List<int>, np.ndarray()\n",
    "            Label for each trial\n",
    "        n_iter : int\n",
    "            Number of permutations\n",
    "        n_pseudo : int\n",
    "            How many trials belong to one bin (aka pseudo-trial)\n",
    "        \"\"\"\n",
    "\n",
    "        self.labels = np.array(labels)\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.classes, self.n_trials = np.unique(labels, return_counts=True)\n",
    "        self.n_classes = self.classes.shape[0]\n",
    "        self.n_pseudo = n_pseudo\n",
    "        self._compute_pseudo_info()\n",
    "\n",
    "    def _compute_pseudo_info(self):\n",
    "        \"\"\"\n",
    "        Compute indices and labels for the pseudo-trial matrix\n",
    "        The pseudo-trial matrix is the resulting matrix *after* having grouped the data into\n",
    "        randomly permuted pseudo-trial bins and averaging trials within each bin. Thus, no\n",
    "        additional permutation is necessary for pseudo-trial indices, which are somewhat trivial.\n",
    "        \"\"\"\n",
    "        self.ind_pseudo_test = np.full((self.n_classes, 1, self.n_pseudo),\n",
    "                                       np.nan, dtype=np.int)\n",
    "        self.labels_pseudo_test = np.full((self.n_classes, 1, self.n_pseudo),\n",
    "                                          np.nan, dtype=np.int)\n",
    "        for c1 in range(self.n_classes):\n",
    "            range_c1 = range(c1*self.n_pseudo, (c1+1)*self.n_pseudo)\n",
    "\n",
    "            self.ind_pseudo_test[c1, 0, :self.n_pseudo] = range_c1\n",
    "            self.labels_pseudo_test[c1, 0, :self.n_pseudo] = self.classes[c1] * np.ones(self.n_pseudo)\n",
    "        self.ind_pseudo_train = []\n",
    "        self.labels_pseudo_train = []\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Generator function for the cross-validation object. Each fold corresponds to a new random\n",
    "        grouping of trials into pseudo-trials.\n",
    "        \"\"\"\n",
    "        _ind_test = np.full(self.n_classes*self.n_pseudo, np.nan, dtype=np.object)\n",
    "        _ind_train = []\n",
    "        for perm in range(self.n_iter):\n",
    "            for c1 in range(self.n_classes):  # separate permutation for each class\n",
    "                prm = np.array(np.array_split(np.random.permutation(self.n_trials[c1]), self.n_pseudo))\n",
    "                ind = prm + np.sum(self.n_trials[:c1])\n",
    "                for i, j in enumerate(range(c1*self.n_pseudo, (c1+1)*self.n_pseudo)):\n",
    "                    _ind_test[j] = ind[i]\n",
    "            yield _ind_train, _ind_test\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_iter\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None):\n",
    "        return self.n_iter\n",
    "\n",
    "\n",
    "class ShuffleBinWithinTest:\n",
    "\n",
    "    def __init__(self, labels, n_iter=10, n_pseudo=5):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List<int>, np.ndarray()\n",
    "            Label for each trial\n",
    "        n_iter : int\n",
    "            Number of permutations\n",
    "        n_pseudo : int\n",
    "            How many trials belong to one bin (aka pseudo-trial)\n",
    "        \"\"\"\n",
    "\n",
    "        self.labels = np.array(labels)\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.classes, self.n_trials = np.unique(labels, return_counts=True)\n",
    "        self.n_classes = self.classes.shape[0]\n",
    "        self.n_pseudo = n_pseudo\n",
    "        self._compute_pseudo_info()\n",
    "\n",
    "    def _compute_pseudo_info(self):\n",
    "        \"\"\"\n",
    "        Compute indices and labels for the pseudo-trial matrix\n",
    "        The pseudo-trial matrix is the resulting matrix *after* having grouped the data into\n",
    "        randomly permuted pseudo-trial bins and averaging trials within each bin. Thus, no\n",
    "        additional permutation is necessary for pseudo-trial indices, which are somewhat trivial.\n",
    "        \"\"\"\n",
    "        self.ind_pseudo_test = np.full((self.n_classes, 1, self.n_pseudo),\n",
    "                                       np.nan, dtype=np.int)\n",
    "        self.labels_pseudo_test = np.full((self.n_classes, 1, self.n_pseudo),\n",
    "                                          np.nan, dtype=np.int)\n",
    "        for c1 in range(self.n_classes):\n",
    "            range_c1 = range(c1*self.n_pseudo, (c1+1)*self.n_pseudo)\n",
    "\n",
    "            self.ind_pseudo_test[c1, 0, :self.n_pseudo] = range_c1\n",
    "            self.labels_pseudo_test[c1, 0, :self.n_pseudo] = self.classes[c1] * np.ones(self.n_pseudo)\n",
    "        self.ind_pseudo_train = []\n",
    "        self.labels_pseudo_train = []\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Generator function for the cross-validation object. Each fold corresponds to a new random\n",
    "        grouping of trials into pseudo-trials.\n",
    "        \"\"\"\n",
    "        _ind_test = np.full(self.n_classes*self.n_pseudo, np.nan, dtype=np.object)\n",
    "        _ind_train = []\n",
    "        for perm in range(self.n_iter):\n",
    "            for c1 in range(self.n_classes):  # separate permutation for each class\n",
    "                prm = np.array(np.array_split(range(self.n_trials[c1]), self.n_pseudo))\n",
    "                ind = prm + np.sum(self.n_trials[:c1])\n",
    "                for i, j in enumerate(range(c1*self.n_pseudo, (c1+1)*self.n_pseudo)):\n",
    "                    _ind_test[j] = ind[i]\n",
    "            yield _ind_train, _ind_test\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_iter\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None):\n",
    "        return self.n_iter\n",
    "\n",
    "class XClassSplit():\n",
    "\n",
    "    def __init__(self, runs, sets):\n",
    "        self.sets = np.atleast_2d(sets)\n",
    "        self.runs = np.array(runs, dtype=np.int)\n",
    "\n",
    "        self.unique_runs = np.unique(self.runs)\n",
    "        self.unique_sets = np.atleast_2d([np.unique(s) for s in self.sets])\n",
    "        self.n = sum([len(s) * len(self.unique_runs) for s in self.unique_sets])\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        for s, set in enumerate(self.sets):\n",
    "            for set_id in self.unique_sets[s]:\n",
    "                for run in self.unique_runs:\n",
    "                    test_index = np.where((set == set_id) & (self.runs == run))[0]\n",
    "                    train_index = np.where((set != set_id) & (self.runs != run))[0]\n",
    "                    yield train_index, test_index\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None):\n",
    "        return self.n\n",
    "\n",
    "\n",
    "# class ExhaustiveLeave2Out:\n",
    "#\n",
    "#     def __init__(self, labels):\n",
    "#         self.labels = labels\n",
    "#         self.classes = np.unique(self.labels)\n",
    "#         self.n_samples = len(self.labels)\n",
    "#         n_samples1 = np.sum(self.labels == self.classes[0])\n",
    "#         n_samples2 = np.sum(self.labels == self.classes[1])\n",
    "#         self.n_iter = 2 * n_samples1 * n_samples2\n",
    "#\n",
    "#     def __iter__(self):\n",
    "#         for i, l in enumerate(self.labels):\n",
    "#             other_class = self.classes[0] if l == self.classes[1] else self.classes[1]\n",
    "#             ind_other_class = np.where(self.labels == other_class)[0]\n",
    "#             for i in ind_other_class:\n",
    "#                 test_ind = [i, i]\n",
    "#                 train_ind = np.setdiff1d(range(self.n_samples), test_ind)\n",
    "#                 yield train_ind, test_ind\n",
    "#\n",
    "#     def split(self, X, y=None):\n",
    "#         return self.__iter__()\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return self.n_iter\n",
    "#\n",
    "#     def get_n_splits(self):\n",
    "#         return self.n_iter\n",
    "\n",
    "\n",
    "class SuperExhaustiveLeaveNOut:\n",
    "\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __iter__(self, y):\n",
    "        for test_ind in combinations(range(len(y)), self.N):\n",
    "            train_ind = np.setdiff1d(range(len(y)), test_ind)\n",
    "            yield train_ind, np.array(test_ind)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        return self.__iter__(y)\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return int(binom(len(y), self.N))\n",
    "\n",
    "\n",
    "class ExhaustiveLeave2Out:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __iter__(self, y):\n",
    "        classes = np.unique(y)\n",
    "        ind1 = np.where(y == classes[0])[0]\n",
    "        ind2 = np.where(y == classes[1])[0]\n",
    "        for test_ind in product(ind1, ind2):\n",
    "            train_ind = np.setdiff1d(range(len(y)), test_ind)\n",
    "            yield train_ind, np.array(test_ind)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        return self.__iter__(y)\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        classes = np.unique(y)\n",
    "        n_samples1 = np.sum(y == classes[0])\n",
    "        n_samples2 = np.sum(y == classes[1])\n",
    "        return n_samples1 * n_samples2\n",
    "\n",
    "\n",
    "class SubsetLeave2Out:\n",
    "\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def __iter__(self, y):\n",
    "        classes = np.unique(y)\n",
    "        ind1 = np.where(y == classes[0])[0]\n",
    "        ind2 = np.where(y == classes[1])[0]\n",
    "        combos = list(product(ind1, ind2))\n",
    "        order = np.random.choice(len(combos), self.n_splits, replace=False)\n",
    "        for i in range(self.n_splits):\n",
    "            test_ind = np.array(combos[order[i]])\n",
    "            train_ind = np.setdiff1d(range(len(y)), test_ind)\n",
    "            yield train_ind, test_ind\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        return self.__iter__(y)\n",
    "\n",
    "    def get_n_splits(self, X, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "\n",
    "class ProxyCV:\n",
    "\n",
    "    def __init__(self, train_ind, test_ind):\n",
    "        self.train_ind = train_ind\n",
    "        self.test_ind = test_ind\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self.train_ind, self.test_ind\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class DummyCV:\n",
    "\n",
    "    def __init__(self, n_samples):\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield list(range(self.n_samples)), []\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    y = np.array(['r1_A1', 'r1_A2', 'r1_B1', 'r1_B2','r2_A1', 'r2_A2', 'r2_B1', 'r2_B2','r3_A1', 'r3_A2', 'r3_B1', 'r3_B2'])\n",
    "\n",
    "    sets = [[0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0]]\n",
    "    runs = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]\n",
    "    cv = XClassSplit(runs, sets)\n",
    "\n",
    "    for train, test in cv:\n",
    "        print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        print(\"y_TRAIN:\", y_train, \"y_TEST:\", y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
